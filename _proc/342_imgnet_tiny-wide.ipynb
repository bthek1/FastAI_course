{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: imgnet_tiny-wide.html\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "title: Tiny Imagenet\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import shutil,timm,os,torch,random,datasets,math\n",
    "import fastcore.all as fc, numpy as np, matplotlib as mpl, matplotlib.pyplot as plt\n",
    "import k_diffusion as K, torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from pathlib import Path\n",
    "from torch.nn import init\n",
    "from fastcore.foundation import L\n",
    "from torch import nn,tensor\n",
    "from operator import itemgetter\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from functools import partial\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import optim\n",
    "from torchvision.io import read_image,ImageReadMode\n",
    "from glob import glob\n",
    "\n",
    "    \n",
    "from fastAIcourse.datasets import *\n",
    "from fastAIcourse.conv import *\n",
    "from fastAIcourse.learner import *\n",
    "from fastAIcourse.activations import *\n",
    "from fastAIcourse.init import *\n",
    "from fastAIcourse.sgd import *\n",
    "from fastAIcourse.resnet import *\n",
    "from fastAIcourse.augment import *\n",
    "from fastAIcourse.accel import *\n",
    "from fastAIcourse.training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=5, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['figure.dpi'] = 70\n",
    "\n",
    "set_seed(42)\n",
    "if fc.defaults.cpus>8: fc.defaults.cpus=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "path_data = Path('Data')\n",
    "path_data.mkdir(exist_ok=True)\n",
    "path = path_data/'tiny-imagenet-200'\n",
    "\n",
    "url = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
    "if not path.exists():\n",
    "    path_zip = fc.urlsave(url, path_data)\n",
    "    shutil.unpack_archive('Data/tiny-imagenet-200.zip', 'data')\n",
    "\n",
    "bs = 512\n",
    "\n",
    "class TinyDS:\n",
    "    def __init__(self, path):\n",
    "        self.path = Path(path)\n",
    "        self.files = glob(str(path/'**/*.JPEG'), recursive=True)\n",
    "    def __len__(self): return len(self.files)\n",
    "    def __getitem__(self, i): return self.files[i],Path(self.files[i]).parent.parent.name\n",
    "\n",
    "tds = TinyDS(path/'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Data/tiny-imagenet-200/train/n02074367/images/n02074367_322.JPEG',\n",
       " 'n02074367')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "path_anno = path/'val'/'val_annotations.txt'\n",
    "anno = dict(o.split('\\t')[:2] for o in path_anno.read_text().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class TinyValDS(TinyDS):\n",
    "    def __getitem__(self, i): return self.files[i],anno[os.path.basename(self.files[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "vds = TinyValDS(path/'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Data/tiny-imagenet-200/val/images/val_240.JPEG', 'n02883205')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class TfmDS:\n",
    "    def __init__(self, ds, tfmx=fc.noop, tfmy=fc.noop): self.ds,self.tfmx,self.tfmy = ds,tfmx,tfmy\n",
    "    def __len__(self): return len(self.ds)\n",
    "    def __getitem__(self, i):\n",
    "        x,y = self.ds[i]\n",
    "        return self.tfmx(x),self.tfmy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "id2str = (path/'wnids.txt').read_text().splitlines()\n",
    "str2id = {v:k for k,v in enumerate(id2str)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "xmean,xstd = (tensor([0.47565, 0.40303, 0.31555]), tensor([0.28858, 0.24402, 0.26615]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def tfmx(x):\n",
    "    img = read_image(x, mode=ImageReadMode.RGB)/255\n",
    "    return (img-xmean[:,None,None])/xstd[:,None,None]\n",
    "\n",
    "def tfmy(y): return tensor(str2id[y])\n",
    "\n",
    "tfm_tds = TfmDS(tds, tfmx, tfmy)\n",
    "tfm_vds = TfmDS(vds, tfmx, tfmy)\n",
    "\n",
    "def denorm(x): return (x*xstd[:,None,None]+xmean[:,None,None]).clip(0,1)\n",
    "\n",
    "all_synsets = [o.split('\\t') for o in (path/'words.txt').read_text().splitlines()]\n",
    "synsets = {k:v.split(',', maxsplit=1)[0] for k,v in all_synsets if k in id2str}\n",
    "\n",
    "dls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def tfm_batch(b, tfm_x=fc.noop, tfm_y = fc.noop): return tfm_x(b[0]),tfm_y(b[1])\n",
    "\n",
    "tfms = nn.Sequential(T.Pad(4), T.RandomCrop(64),\n",
    "                     T.RandomHorizontalFlip(),\n",
    "                     RandErase())\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)\n",
    "\n",
    "act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\n",
    "iw = partial(init_weights, leaky=0.1)\n",
    "\n",
    "nfs = (32,64,128,256,512,1024)\n",
    "\n",
    "def get_dropmodel(act=act_gr, nfs=nfs, norm=nn.BatchNorm2d, drop=0.1):\n",
    "    layers = [nn.Conv2d(3, nfs[0], 5, padding=2)]\n",
    "#     layers += [ResBlock(nfs[0], nfs[0], ks=3, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n",
    "               for i in range(len(nfs)-1)]\n",
    "    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n",
    "    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n",
    "    return nn.Sequential(*layers).apply(iw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def res_blocks(n_bk, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n",
    "    return nn.Sequential(*[\n",
    "        ResBlock(ni if i==0 else nf, nf, stride=stride if i==n_bk-1 else 1, ks=ks, act=act, norm=norm)\n",
    "        for i in range(n_bk)])\n",
    "\n",
    "nbks = (3,2,2,1,1)\n",
    "\n",
    "def get_dropmodel(act=act_gr, nfs=nfs, nbks=nbks, norm=nn.BatchNorm2d, drop=0.2):\n",
    "    layers = [ResBlock(3, nfs[0], ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [res_blocks(nbks[i], nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n",
    "               for i in range(len(nfs)-1)]\n",
    "    layers += [nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n",
    "    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n",
    "    return nn.Sequential(*layers).apply(iw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "opt_func = partial(optim.AdamW, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=True), MixedPrecision()]\n",
    "\n",
    "epochs = 25\n",
    "lr = 3e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "learn = Learner(get_dropmodel(), dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "aug_tfms = nn.Sequential(T.Pad(4), T.RandomCrop(64),\n",
    "                     T.RandomHorizontalFlip(),\n",
    "                     T.TrivialAugmentWide())\n",
    "\n",
    "norm_tfm = T.Normalize(xmean, xstd)\n",
    "erase_tfm = RandErase()\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def tfmx(x, aug=False):\n",
    "    x = Image.open(x).convert('RGB')\n",
    "    if aug: x = aug_tfms(x)\n",
    "    x = TF.to_tensor(x)\n",
    "    x = norm_tfm(x)\n",
    "    if aug: x = erase_tfm(x[None])[0]\n",
    "    return x\n",
    "\n",
    "tfm_tds = TfmDS(tds, partial(tfmx, aug=True), tfmy)\n",
    "tfm_vds = TfmDS(vds, tfmx, tfmy)\n",
    "\n",
    "dls = DataLoaders(*get_dls(tfm_tds, tfm_vds, bs=bs, num_workers=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def conv(ni, nf, ks=3, stride=1, act=nn.ReLU, norm=None, bias=True):\n",
    "    layers = []\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def _conv_block(ni, nf, stride, act=act_gr, norm=None, ks=3):\n",
    "    return nn.Sequential(conv(ni, nf, stride=1     , act=act, norm=norm, ks=ks),\n",
    "                         conv(nf, nf, stride=stride, act=act, norm=norm, ks=ks))\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n",
    "        super().__init__()\n",
    "        self.convs = _conv_block(ni, nf, stride, act=act, ks=ks, norm=norm)\n",
    "        self.idconv = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=None, norm=norm)\n",
    "        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n",
    "\n",
    "    def forward(self, x): return self.convs(x) + self.idconv(self.pool(x))\n",
    "\n",
    "def get_dropmodel(act=act_gr, nfs=nfs, nbks=nbks, norm=nn.BatchNorm2d, drop=0.2):\n",
    "    layers = [nn.Conv2d(3, nfs[0], 5, padding=2)]\n",
    "    layers += [res_blocks(nbks[i], nfs[i], nfs[i+1], act=act, norm=norm, stride=2)\n",
    "               for i in range(len(nfs)-1)]\n",
    "    layers += [act_gr(), norm(nfs[-1]), nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Dropout(drop)]\n",
    "    layers += [nn.Linear(nfs[-1], 200, bias=False), nn.BatchNorm1d(200)]\n",
    "    return nn.Sequential(*layers).apply(iw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "lr = 0.1\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "model = get_dropmodel(nbks=(1,2,8,2,2), nfs=(32, 64, 128, 512, 1024, 1536), drop=0.1)\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/50 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='2' class='' max='196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.02% [2/196 02:32&lt;4:06:46 5.598]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "torch.save(learn.model, 'models/inettiny-wide-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
