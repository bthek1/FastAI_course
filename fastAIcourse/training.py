"""Mini Batch"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/140_minibatch_training.ipynb.

# %% auto 0
__all__ = ['Model', 'log_softmax', 'logsumexp', 'nll', 'accuracy', 'report', 'MLP', 'fit', 'MyModule', 'SequentialModel',
           'Optimizer', 'get_model', 'Dataset', 'DataLoader', 'Sampler', 'BatchSampler', 'collate', 'get_dls']

# %% ../nbs/140_minibatch_training.ipynb 2
import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
from pathlib import Path
from torch import tensor,nn
import torch.nn.functional as F

# %% ../nbs/140_minibatch_training.ipynb 7
class Model(nn.Module):
    def __init__(self, n_in, nh, n_out):
        super().__init__()
        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]
        
    def __call__(self, x):
        for l in self.layers: x = l(x)
        return x

# %% ../nbs/140_minibatch_training.ipynb 11
def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()

# %% ../nbs/140_minibatch_training.ipynb 14
def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()

# %% ../nbs/140_minibatch_training.ipynb 16
def logsumexp(x):
    m = x.max(-1)[0]
    return m + (x-m[:,None]).exp().sum(-1).log()

# %% ../nbs/140_minibatch_training.ipynb 18
def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)

# %% ../nbs/140_minibatch_training.ipynb 24
def nll(input, target): return -input[range(target.shape[0]), target].mean()

# %% ../nbs/140_minibatch_training.ipynb 38
def accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()

# %% ../nbs/140_minibatch_training.ipynb 41
def report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')

# %% ../nbs/140_minibatch_training.ipynb 51
class MLP(nn.Module):
    def __init__(self, n_in, nh, n_out):
        super().__init__()
        self.l1 = nn.Linear(n_in,nh)
        self.l2 = nn.Linear(nh,n_out)
        self.relu = nn.ReLU()
        
    def forward(self, x): return self.l2(self.relu(self.l1(x)))

# %% ../nbs/140_minibatch_training.ipynb 56
def fit():
    for epoch in range(epochs):
        for i in range(0, n, bs):
            s = slice(i, min(n,i+bs))
            xb,yb = x_train[s],y_train[s]
            preds = model(xb)
            loss = loss_func(preds, yb)
            loss.backward()
            with torch.no_grad():
                for p in model.parameters(): p -= p.grad * lr
                model.zero_grad()
        report(loss, preds, yb)

# %% ../nbs/140_minibatch_training.ipynb 59
class MyModule:
    def __init__(self, n_in, nh, n_out):
        self._modules = {}
        self.l1 = nn.Linear(n_in,nh)
        self.l2 = nn.Linear(nh,n_out)

    def __setattr__(self,k,v):
        if not k.startswith("_"): self._modules[k] = v
        super().__setattr__(k,v)

    def __repr__(self): return f'{self._modules}'
    
    def parameters(self):
        for l in self._modules.values(): yield from l.parameters()

# %% ../nbs/140_minibatch_training.ipynb 63
from functools import reduce

# %% ../nbs/140_minibatch_training.ipynb 66
class Model(nn.Module):
    def __init__(self, layers):
        super().__init__()
        self.layers = layers
        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)

    def forward(self, x): return reduce(lambda val,layer: layer(val), self.layers, x)

# %% ../nbs/140_minibatch_training.ipynb 71
class SequentialModel(nn.Module):
    def __init__(self, layers):
        super().__init__()
        self.layers = nn.ModuleList(layers)
        
    def forward(self, x):
        for l in self.layers: x = l(x)
        return x

# %% ../nbs/140_minibatch_training.ipynb 80
class Optimizer():
    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr

    def step(self):
        with torch.no_grad():
            for p in self.params: p -= p.grad * self.lr

    def zero_grad(self):
        for p in self.params: p.grad.data.zero_()

# %% ../nbs/140_minibatch_training.ipynb 85
from torch import optim

# %% ../nbs/140_minibatch_training.ipynb 86
def get_model():
    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))
    return model, optim.SGD(model.parameters(), lr=lr)

# %% ../nbs/140_minibatch_training.ipynb 92
class Dataset():
    def __init__(self, x, y): self.x,self.y = x,y
    def __len__(self): return len(self.x)
    def __getitem__(self, i): return self.x[i],self.y[i]

# %% ../nbs/140_minibatch_training.ipynb 99
class DataLoader():
    def __init__(self, ds, bs): self.ds,self.bs = ds,bs
    def __iter__(self):
        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]

# %% ../nbs/140_minibatch_training.ipynb 105
def fit():
    for epoch in range(epochs):
        for xb,yb in train_dl:
            preds = model(xb)
            loss = loss_func(preds, yb)
            loss.backward()
            opt.step()
            opt.zero_grad()
        report(loss, preds, yb)

# %% ../nbs/140_minibatch_training.ipynb 109
import random

# %% ../nbs/140_minibatch_training.ipynb 110
class Sampler():
    def __init__(self, ds, shuffle=False): self.n,self.shuffle = len(ds),shuffle
    def __iter__(self):
        res = list(range(self.n))
        if self.shuffle: random.shuffle(res)
        return iter(res)

# %% ../nbs/140_minibatch_training.ipynb 111
from itertools import islice

# %% ../nbs/140_minibatch_training.ipynb 116
import fastcore.all as fc

# %% ../nbs/140_minibatch_training.ipynb 117
class BatchSampler():
    def __init__(self, sampler, bs, drop_last=False): fc.store_attr()
    def __iter__(self): yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)

# %% ../nbs/140_minibatch_training.ipynb 119
def collate(b):
    xs,ys = zip(*b)
    return torch.stack(xs),torch.stack(ys)

# %% ../nbs/140_minibatch_training.ipynb 120
class DataLoader():
    def __init__(self, ds, batchs, collate_fn=collate): fc.store_attr()
    def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batchs)

# %% ../nbs/140_minibatch_training.ipynb 128
import torch.multiprocessing as mp
from fastcore.basics import store_attr

# %% ../nbs/140_minibatch_training.ipynb 132
class DataLoader():
    def __init__(self, ds, batchs, n_workers=1, collate_fn=collate): fc.store_attr()
    def __iter__(self):
        with mp.Pool(self.n_workers) as ex: yield from ex.map(self.ds.__getitem__, iter(self.batchs))

# %% ../nbs/140_minibatch_training.ipynb 136
from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler

# %% ../nbs/140_minibatch_training.ipynb 152
def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
    for epoch in range(epochs):
        model.train()
        for xb,yb in train_dl:
            loss = loss_func(model(xb), yb)
            loss.backward()
            opt.step()
            opt.zero_grad()

        model.eval()
        with torch.no_grad():
            tot_loss,tot_acc,count = 0.,0.,0
            for xb,yb in valid_dl:
                pred = model(xb)
                n = len(xb)
                count += n
                tot_loss += loss_func(pred,yb).item()*n
                tot_acc  += accuracy (pred,yb).item()*n
        print(epoch, tot_loss/count, tot_acc/count)
    return tot_loss/count, tot_acc/count

# %% ../nbs/140_minibatch_training.ipynb 153
def get_dls(train_ds, valid_ds, bs, **kwargs):
    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),
            DataLoader(valid_ds, batch_size=bs*2, **kwargs))
